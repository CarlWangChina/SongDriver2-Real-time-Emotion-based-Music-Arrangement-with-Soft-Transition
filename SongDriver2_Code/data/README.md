# Obtain training data

- This paper does not make any innovative points or contribution points in terms of dataset.
- Therefore, you can refer to the following open source innovation / contribution work of other researchers to get the training data by yourself.

## Open source contribution work from other researchers
- Chapter 4 of the Paper gives the source, citation and download links of the dataset, and we use open source emotion labeling datasets from other researchers.
- Chapter 8 of the Supplementary gives the detailed step-by-step methods for processing the dataset and the data representation, and we used other researchers' open-source audio-to-midi methods such as Onsets & Frames, Harvest method, etc. to obtain the training data.

## Take more suitable data representation for your own model
- The Paper and Supplementary spend nearly 2 pages to explain the training data acquisition, which is very detailed, thus other researchers can follow these steps to obtain their own training data and be more free to take more suitable data representation for their own models, such as midi, musicxml, REMI, etc.












